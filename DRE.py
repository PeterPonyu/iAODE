
import numpy as np
import pandas as pd
from sklearn.metrics import pairwise_distances
from scipy.stats import spearmanr
import time
import warnings
from typing import Dict, Tuple

class DimensionalityReductionEvaluator:
    """
    ç²¾ç‚¼ç‰ˆé™ç»´è´¨é‡è¯„ä¼°å™¨
    
    ä¸“æ³¨äºä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼š
    - distance_correlation: è·ç¦»ç›¸å…³æ€§ (å…¨å±€ç»“æ„ä¿ç•™)
    - Q_global: å…¨å±€è´¨é‡æŒ‡æ ‡
    - Q_local: å±€éƒ¨è´¨é‡æŒ‡æ ‡
    
    ç‰¹ç‚¹ï¼š
    - é«˜æ•ˆçš„çŸ¢é‡åŒ–è®¡ç®—
    - ä¸“æ³¨äºæœ€é‡è¦çš„è¯„ä¼°æŒ‡æ ‡
    - ä¸å•ç»†èƒè¯„ä¼°æ¡†æ¶äº’è¡¥
    """
    
    def __init__(self, verbose=True):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        å‚æ•°:
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.verbose = verbose
        
    def _log(self, message):
        if self.verbose:
            print(message)
    
    def _validate_inputs(self, X_high, X_low, k):
        """éªŒè¯è¾“å…¥å‚æ•°"""
        if not isinstance(X_high, np.ndarray) or not isinstance(X_low, np.ndarray):
            raise TypeError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯numpyæ•°ç»„")
        
        if X_high.shape[0] != X_low.shape[0]:
            raise ValueError(f"é«˜ç»´å’Œä½ç»´æ•°æ®æ ·æœ¬æ•°å¿…é¡»ç›¸åŒ: {X_high.shape[0]} vs {X_low.shape[0]}")
        
        if k >= X_high.shape[0]:
            raise ValueError(f"kå€¼({k})ä¸èƒ½å¤§äºæˆ–ç­‰äºæ ·æœ¬æ•°({X_high.shape[0]})")
            
        if X_high.ndim != 2 or X_low.ndim != 2:
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯äºŒç»´æ•°ç»„")
    
    # ==================== 1. è·ç¦»ç›¸å…³æ€§è®¡ç®— ====================
    
    def distance_correlation_score(self, X_high, X_low):
        """
        è®¡ç®—è·ç¦»ç›¸å…³æ€§ (Spearmanç›¸å…³)
        è¯„ä¼°é«˜ç»´å’Œä½ç»´ç©ºé—´ä¸­è·ç¦»çš„å•è°ƒå…³ç³»
        
        å‚æ•°:
            X_high: é«˜ç»´ç©ºé—´æ•°æ®
            X_low: ä½ç»´ç©ºé—´æ•°æ®
            
        è¿”å›:
            float: è·ç¦»ç›¸å…³æ€§åˆ†æ•° (æ¥è¿‘1è¡¨ç¤ºå…¨å±€ç»“æ„ä¿ç•™è‰¯å¥½)
        """
        try:
            self._log("è®¡ç®—è·ç¦»çŸ©é˜µ...")
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            D_high = pairwise_distances(X_high)
            D_low = pairwise_distances(X_low)
            
            # è®¡ç®—Spearmanç›¸å…³æ€§
            distance_corr, _ = spearmanr(D_high.flatten(), D_low.flatten())
            
            return distance_corr if not np.isnan(distance_corr) else 0.0
            
        except Exception as e:
            warnings.warn(f"è·ç¦»ç›¸å…³æ€§è®¡ç®—å‡ºé”™: {e}")
            return 0.0
    
    # ==================== 2. æ’åçŸ©é˜µè®¡ç®— ====================
    
    def get_ranking_matrix(self, distance_matrix):
        """
        è®¡ç®—æ’åçŸ©é˜µï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
            distance_matrix: è·ç¦»çŸ©é˜µ
            
        è¿”å›:
            ranking_matrix: æ’åçŸ©é˜µ
        """
        try:
            n = len(distance_matrix)
            
            # ä½¿ç”¨argsortç›´æ¥è·å¾—æ’åï¼Œé¿å…å¾ªç¯
            sorted_indices = np.argsort(distance_matrix, axis=1)
            
            # åˆ›å»ºæ’åçŸ©é˜µ
            ranking_matrix = np.zeros((n, n), dtype=np.int32)
            
            # çŸ¢é‡åŒ–æ“ä½œï¼šä¸ºæ¯ä¸€è¡Œåˆ†é…æ’å
            for i in range(n):
                ranking_matrix[i, sorted_indices[i]] = np.arange(n)
            
            # æ’é™¤è‡ªèº«ï¼ˆå°†å¯¹è§’çº¿è®¾ä¸º0ï¼Œå…¶ä»–æ’åå‡1ï¼‰
            mask = np.eye(n, dtype=bool)
            ranking_matrix[~mask] = ranking_matrix[~mask] - 1
            ranking_matrix[mask] = 0
            
            return ranking_matrix
            
        except Exception as e:
            warnings.warn(f"æ’åçŸ©é˜µè®¡ç®—å‡ºé”™: {e}")
            return np.zeros((len(distance_matrix), len(distance_matrix)), dtype=np.int32)
    
    # ==================== 3. å…±æ’åçŸ©é˜µè®¡ç®— ====================
    
    def get_coranking_matrix(self, rank_high, rank_low):
        """
        è®¡ç®—å…±æ’åçŸ©é˜µï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
            rank_high: é«˜ç»´ç©ºé—´æ’åçŸ©é˜µ
            rank_low: ä½ç»´ç©ºé—´æ’åçŸ©é˜µ
            
        è¿”å›:
            coranking_matrix: å…±æ’åçŸ©é˜µ
        """
        try:
            n = len(rank_high)
            corank = np.zeros((n-1, n-1), dtype=np.int32)
            
            # çŸ¢é‡åŒ–æ“ä½œï¼šä½¿ç”¨numpyçš„é«˜çº§ç´¢å¼•
            mask = (rank_high > 0) & (rank_low > 0)
            valid_high = rank_high[mask] - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
            valid_low = rank_low[mask] - 1
            
            # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            valid_mask = (valid_high < n-1) & (valid_low < n-1)
            valid_high = valid_high[valid_mask]
            valid_low = valid_low[valid_mask]
            
            # ä½¿ç”¨np.add.atè¿›è¡Œç´¯åŠ 
            np.add.at(corank, (valid_high, valid_low), 1)
            
            return corank
            
        except Exception as e:
            warnings.warn(f"å…±æ’åçŸ©é˜µè®¡ç®—å‡ºé”™: {e}")
            n = len(rank_high)
            return np.zeros((n-1, n-1), dtype=np.int32)
    
    # ==================== 4. QæŒ‡æ ‡è®¡ç®— ====================
    
    def compute_qnx_series(self, corank):
        """
        è®¡ç®—Q_NXåºåˆ—
        
        å‚æ•°:
            corank: å…±æ’åçŸ©é˜µ
            
        è¿”å›:
            np.ndarray: Q_NXå€¼åºåˆ—
        """
        try:
            n = corank.shape[0] + 1
            qnx_values = []
            
            Qnx_cum = 0
            
            for K in range(1, n-1):
                # è®¡ç®—å¢é‡
                if K-1 < corank.shape[0]:
                    intrusions = np.sum(corank[:K, K-1]) if K-1 < corank.shape[1] else 0
                    extrusions = np.sum(corank[K-1, :K]) if K-1 < corank.shape[0] else 0
                    diagonal = corank[K-1, K-1] if K-1 < min(corank.shape) else 0
                    
                    Qnx_increment = intrusions + extrusions - diagonal
                    Qnx_cum += Qnx_increment
                    
                    # å½’ä¸€åŒ–
                    qnx_normalized = Qnx_cum / (K * n)
                    qnx_values.append(qnx_normalized)
            
            return np.array(qnx_values)
            
        except Exception as e:
            warnings.warn(f"Q_NXåºåˆ—è®¡ç®—å‡ºé”™: {e}")
            return np.array([0.0])
    
    def get_q_local_global(self, qnx_values):
        """
        è®¡ç®—å±€éƒ¨å’Œå…¨å±€è´¨é‡æ ‡é‡
        
        å‚æ•°:
            qnx_values: Q_NXå€¼åºåˆ—
            
        è¿”å›:
            tuple: (Q_local, Q_global, K_max)
        """
        try:
            if len(qnx_values) == 0:
                return 0.0, 0.0, 1
            
            # è®¡ç®—LCMC (Local Continuity Meta-Criterion)
            lcmc = np.copy(qnx_values)
            N = len(qnx_values)
            
            for j in range(N):
                lcmc[j] = lcmc[j] - j/N
            
            K_max = np.argmax(lcmc) + 1
            
            # è®¡ç®—Q_localå’ŒQ_global
            if K_max > 0:
                Q_local = np.mean(qnx_values[:K_max])
            else:
                Q_local = qnx_values[0] if len(qnx_values) > 0 else 0.0
                
            if K_max < len(qnx_values):
                Q_global = np.mean(qnx_values[K_max:])
            else:
                Q_global = qnx_values[-1] if len(qnx_values) > 0 else 0.0
            
            return Q_local, Q_global, K_max
            
        except Exception as e:
            warnings.warn(f"QæŒ‡æ ‡è®¡ç®—å‡ºé”™: {e}")
            return 0.0, 0.0, 1
    
    # ==================== 5. ç»¼åˆè¯„ä¼°æ¡†æ¶ ====================
    
    def comprehensive_evaluation(self, X_high, X_low, k=10):
        """
        ç»¼åˆé™ç»´è´¨é‡è¯„ä¼°
        
        å‚æ•°:
            X_high: é«˜ç»´ç©ºé—´æ•°æ®, shape=(n_samples, n_features_high)
            X_low: ä½ç»´ç©ºé—´æ•°æ®, shape=(n_samples, n_features_low)
            k: è€ƒè™‘çš„è¿‘é‚»æ•°é‡
            
        è¿”å›:
            dict: åŒ…å«æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """        
        # è¾“å…¥éªŒè¯
        self._validate_inputs(X_high, X_low, k)
        
        self._log(f"å¼€å§‹é™ç»´è´¨é‡è¯„ä¼° (æ ·æœ¬æ•°: {X_high.shape[0]}, k={k})...")
        
        results = {}
        
        # 1. è·ç¦»ç›¸å…³æ€§
        self._log("è®¡ç®—è·ç¦»ç›¸å…³æ€§...")
        results['distance_correlation'] = self.distance_correlation_score(X_high, X_low)
        
        # 2. è®¡ç®—æ’åçŸ©é˜µ
        self._log("è®¡ç®—æ’åçŸ©é˜µ...")
        D_high = pairwise_distances(X_high)
        D_low = pairwise_distances(X_low)
        
        rank_high = self.get_ranking_matrix(D_high)
        rank_low = self.get_ranking_matrix(D_low)
        
        # 3. è®¡ç®—å…±æ’åçŸ©é˜µ
        self._log("è®¡ç®—å…±æ’åçŸ©é˜µ...")
        corank = self.get_coranking_matrix(rank_high, rank_low)
        
        # 4. è®¡ç®—QæŒ‡æ ‡
        self._log("è®¡ç®—QæŒ‡æ ‡...")
        qnx_values = self.compute_qnx_series(corank)
        Q_local, Q_global, K_max = self.get_q_local_global(qnx_values)
        
        results['Q_local'] = Q_local
        results['Q_global'] = Q_global
        results['K_max'] = K_max
        
        # è´¨é‡è¯„ä¼°
        overall_quality = np.mean([
            results['distance_correlation'],
            results['Q_local'],
            results['Q_global']
        ])
        results['overall_quality'] = overall_quality
        
        if self.verbose:
            self._print_results(results)
        
        return results
    
    def _print_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        
        print("\n" + "="*60)
        print("              é™ç»´è´¨é‡è¯„ä¼°ç»“æœ")
        print("="*60)
        
        print(f"\nã€æ ¸å¿ƒè´¨é‡æŒ‡æ ‡ã€‘")
        print(f"  è·ç¦»ç›¸å…³æ€§: {results['distance_correlation']:.4f} â˜…")
        print(f"    â””â”€ æ¥è¿‘1è¡¨ç¤ºå…¨å±€ç»“æ„ä¿ç•™è‰¯å¥½")
        
        print(f"\n  å±€éƒ¨è´¨é‡(Q_local): {results['Q_local']:.4f} â˜…")
        print(f"    â””â”€ æ¥è¿‘1è¡¨ç¤ºå±€éƒ¨ç»“æ„ä¿ç•™è‰¯å¥½")
        
        print(f"\n  å…¨å±€è´¨é‡(Q_global): {results['Q_global']:.4f} â˜…")
        print(f"    â””â”€ æ¥è¿‘1è¡¨ç¤ºå…¨å±€ç»“æ„ä¿ç•™è‰¯å¥½")
        
        print(f"\nã€è¾…åŠ©ä¿¡æ¯ã€‘")
        print(f"  å±€éƒ¨-å…¨å±€åˆ†ç•Œç‚¹(K_max): {results['K_max']}")
        
        # è´¨é‡è¯„ä¼°
        overall_quality = results['overall_quality']
        
        print(f"\nã€ç»¼åˆè¯„ä¼°ã€‘")
        print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {overall_quality:.4f}")
        
        if overall_quality >= 0.8:
            quality_level = "ä¼˜ç§€"
        elif overall_quality >= 0.6:
            quality_level = "è‰¯å¥½"
        elif overall_quality >= 0.4:
            quality_level = "ä¸­ç­‰"
        else:
            quality_level = "éœ€è¦æ”¹è¿›"
            
        print(f"  è´¨é‡ç­‰çº§: {quality_level}")
        
        print("="*60)
    
    def compare_methods(self, method_results_dict, k=10):
        """
        æ¯”è¾ƒä¸åŒé™ç»´æ–¹æ³•çš„æ•ˆæœ
        
        å‚æ•°:
            method_results_dict: {method_name: (X_high, X_low)} å­—å…¸
            k: è€ƒè™‘çš„è¿‘é‚»æ•°é‡
            
        è¿”å›:
            DataFrame: æ¯”è¾ƒç»“æœè¡¨æ ¼
        """
        
        comparison_results = []
        
        for method_name, (X_high, X_low) in method_results_dict.items():
            self._log(f"\nè¯„ä¼°æ–¹æ³•: {method_name}")
            
            # æš‚æ—¶å…³é—­è¯¦ç»†è¾“å‡º
            original_verbose = self.verbose
            self.verbose = False
            
            results = self.comprehensive_evaluation(X_high, X_low, k)
            
            # æ¢å¤è¾“å‡ºè®¾ç½®
            self.verbose = original_verbose
            
            # è®¡ç®—ç»¼åˆåˆ†æ•°
            overall_quality = np.mean([
                results['distance_correlation'],
                results['Q_local'],
                results['Q_global']
            ])
            
            # æ·»åŠ åˆ°æ¯”è¾ƒç»“æœ
            comparison_results.append({
                'Method': method_name,
                'Distance_Correlation': results['distance_correlation'],
                'Q_Local': results['Q_local'],
                'Q_Global': results['Q_global'],
                'Overall_Quality': overall_quality,
            })
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(comparison_results)
        
        # æŒ‰ç»¼åˆè´¨é‡æ’åº
        df = df.sort_values('Overall_Quality', ascending=False)
        
        if self.verbose:
            self._print_comparison_table(df)
        
        return df
    
    def _print_comparison_table(self, df):
        """æ‰“å°æ¯”è¾ƒç»“æœè¡¨æ ¼"""
        
        print(f"\n{'='*90}")
        print(f"                          é™ç»´æ–¹æ³•æ•ˆæœæ¯”è¾ƒ")
        print('='*90)
        
        # è®¾ç½®æ˜¾ç¤ºæ ¼å¼
        pd.set_option('display.float_format', '{:.4f}'.format)
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        
        print(df.to_string(index=False))
        
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {df.iloc[0]['Method']} (ç»¼åˆå¾—åˆ†: {df.iloc[0]['Overall_Quality']:.4f})")
        
        print('='*90)

# ==================== ä¾¿æ·å‡½æ•° ====================

def evaluate_dimensionality_reduction(X_high, X_low, k=10, verbose=True):
    """
    ä¾¿æ·å‡½æ•°ï¼šè¯„ä¼°é™ç»´è´¨é‡
    
    å‚æ•°:
        X_high: é«˜ç»´ç©ºé—´æ•°æ®
        X_low: ä½ç»´ç©ºé—´æ•°æ®
        k: è€ƒè™‘çš„è¿‘é‚»æ•°é‡
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        
    è¿”å›:
        dict: è¯„ä¼°ç»“æœ
    """
    
    evaluator = DimensionalityReductionEvaluator(verbose=verbose)
    return evaluator.comprehensive_evaluation(X_high, X_low, k)

def compare_dimensionality_reduction_methods(method_results_dict, k=10, verbose=True):
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¯”è¾ƒä¸åŒé™ç»´æ–¹æ³•
    
    å‚æ•°:
        method_results_dict: {method_name: (X_high, X_low)} å­—å…¸
        k: è€ƒè™‘çš„è¿‘é‚»æ•°é‡
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        
    è¿”å›:
        DataFrame: æ¯”è¾ƒç»“æœ
    """
    
    evaluator = DimensionalityReductionEvaluator(verbose=verbose)
    return evaluator.compare_methods(method_results_dict, k)