
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from scipy.stats import entropy, skew, kurtosis
from scipy.linalg import svd, norm
import warnings

class SingleCellLatentSpaceEvaluator:
    """
    ä¸“é—¨é’ˆå¯¹å•ç»†èƒæ•°æ®çš„æ½œåœ¨ç©ºé—´è´¨é‡è¯„ä¼°å™¨
    
    ç‰¹åˆ«é€‚ç”¨äºï¼š
    - å•ç»†èƒè½¨è¿¹æ•°æ® (å‘è‚²ã€åˆ†åŒ–ç­‰)
    - å•ç»†èƒç¨³æ€ç¾¤ä½“æ•°æ®
    - æ—¶é—´åºåˆ—å•ç»†èƒæ•°æ®
    
    å…³é”®ç‰¹æ€§ï¼š
    - ä¸ºè½¨è¿¹æ•°æ®è°ƒæ•´äº†æŒ‡æ ‡è§£é‡Š
    - ä½å„å‘åŒæ€§ = å¥½ (å¼ºæ–¹å‘æ€§)
    - ä½å‚ä¸æ¯” = å¥½ (ä¿¡æ¯é›†ä¸­)
    - é«˜è°±è¡°å‡ = å¥½ (ç»´åº¦æ•ˆç‡)
    """
    
    def __init__(self, data_type="trajectory", verbose=True):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        å‚æ•°:
            data_type: "trajectory" æˆ– "steady_state"
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.data_type = data_type
        self.verbose = verbose
        
        # æ ¹æ®æ•°æ®ç±»å‹è°ƒæ•´æœŸæœ›å€¼
        if data_type == "trajectory":
            self.isotropy_preference = "low"      # è½¨è¿¹æœŸæœ›ä½å„å‘åŒæ€§
            self.participation_preference = "low"  # è½¨è¿¹æœŸæœ›ä½å‚ä¸æ¯”
        else:  # steady_state
            self.isotropy_preference = "high"     # ç¨³æ€æœŸæœ›é«˜å„å‘åŒæ€§  
            self.participation_preference = "high" # ç¨³æ€æœŸæœ›é«˜å‚ä¸æ¯”
    
    def _log(self, message):
        if self.verbose:
            print(message)
    
    # ==================== 1. ä¿®æ­£çš„æµå½¢ç»´åº¦ä¸€è‡´æ€§ ====================
    
    def manifold_dimensionality_score_v2(self, latent_space, 
                                        variance_thresholds=[0.8, 0.9, 0.95],
                                        use_multiple_methods=True):
        """
        ä¿®æ­£ç‰ˆæµå½¢ç»´åº¦ä¸€è‡´æ€§è¯„ä¼°
        è§£å†³äº†åŸç‰ˆæœ¬æ‰€æœ‰æ–¹æ³•å¾—åˆ†ç›¸åŒçš„é—®é¢˜
        
        å‚æ•°:
            latent_space: æ½œåœ¨ç©ºé—´åæ ‡
            variance_thresholds: å¤šä¸ªæ–¹å·®é˜ˆå€¼
            use_multiple_methods: æ˜¯å¦ä½¿ç”¨å¤šç§æ–¹æ³•
            
        è¿”å›:
            float: ç»´åº¦æ•ˆç‡åˆ†æ•° (0-1)
        """
        try:
            if latent_space.shape[1] == 1:
                return 1.0
            
            # ä¸­å¿ƒåŒ–æ•°æ®
            centered_data = latent_space - np.mean(latent_space, axis=0)
            
            # PCAåˆ†æ
            pca = PCA().fit(centered_data)
            explained_variance_ratio = pca.explained_variance_ratio_
            explained_variance = pca.explained_variance_
            
            dimension_scores = []
            
            # æ–¹æ³•1ï¼šå¤šé˜ˆå€¼ç»´åº¦æ•ˆç‡
            for threshold in variance_thresholds:
                cumsum = np.cumsum(explained_variance_ratio)
                effective_dims = np.where(cumsum >= threshold)[0]
                
                if len(effective_dims) > 0:
                    effective_dim = effective_dims[0] + 1
                    # ä¿®æ­£çš„æ•ˆç‡è®¡ç®—ï¼šæ›´å°‘ç»´åº¦è¾¾åˆ°é˜ˆå€¼ = æ›´å¥½
                    efficiency = 1.0 - (effective_dim - 1) / (latent_space.shape[1] - 1)
                    dimension_scores.append(efficiency)
            
            # æ–¹æ³•2ï¼šKaiserå‡†åˆ™ç»´åº¦æ•ˆç‡
            normalized_eigenvalues = explained_variance / np.mean(explained_variance)
            kaiser_dim = np.sum(normalized_eigenvalues > 1.0)
            kaiser_efficiency = 1.0 - (kaiser_dim - 1) / (latent_space.shape[1] - 1)
            
            # æ–¹æ³•3ï¼šè‚˜éƒ¨æ³•åˆ™
            if len(explained_variance) > 2:
                ratios = explained_variance[:-1] / explained_variance[1:]
                elbow_dim = np.argmax(ratios) + 1
                elbow_efficiency = 1.0 - (elbow_dim - 1) / (latent_space.shape[1] - 1)
            else:
                elbow_efficiency = 1.0
            
            # æ–¹æ³•4ï¼šè°±è¡°å‡ç‡
            if len(explained_variance) > 1:
                # è®¡ç®—ç‰¹å¾å€¼çš„å¯¹æ•°è¡°å‡
                log_eigenvals = np.log(explained_variance + 1e-10)
                x = np.arange(len(log_eigenvals))
                
                # çº¿æ€§æ‹Ÿåˆæ–œç‡ï¼ˆè¡°å‡ç‡ï¼‰
                if len(x) > 1:
                    slope = np.polyfit(x, log_eigenvals, 1)[0]
                    # æ–œç‡è¶Šè´Ÿï¼Œè¡°å‡è¶Šå¿«ï¼Œç»´åº¦é›†ä¸­åº¦è¶Šå¥½
                    decay_score = 1.0 / (1.0 + np.exp(slope))
                else:
                    decay_score = 0.5
            else:
                decay_score = 0.5
            
            # ç»¼åˆåˆ†æ•°
            if use_multiple_methods:
                all_scores = dimension_scores + [kaiser_efficiency, elbow_efficiency, decay_score]
                final_score = np.mean([s for s in all_scores if s is not None])
            else:
                final_score = np.mean(dimension_scores) if dimension_scores else 0.5
            
            return np.clip(final_score, 0.0, 1.0)
            
        except Exception as e:
            warnings.warn(f"æµå½¢ç»´åº¦ä¸€è‡´æ€§è®¡ç®—å‡ºé”™: {e}")
            return 0.5
    
    # ==================== 2. é«˜æ•ˆå†…åœ¨ç‰¹æ€§æŒ‡æ ‡ ====================
    
    def spectral_decay_rate(self, latent_space):
        """è°±è¡°å‡ç‡ - è¶Šé«˜è¡¨ç¤ºç»´åº¦é›†ä¸­åº¦è¶Šå¥½"""
        try:
            centered_data = latent_space - np.mean(latent_space, axis=0)
            U, s, Vt = svd(centered_data, full_matrices=False)
            eigenvalues = s**2 / (len(latent_space) - 1)
            
            if len(eigenvalues) < 2:
                return 1.0
            
            # æŒ‡æ•°è¡°å‡æ‹Ÿåˆ
            log_eigenvals = np.log(eigenvalues + 1e-10)
            x = np.arange(len(log_eigenvals))
            
            slope, _ = np.polyfit(x, log_eigenvals, 1)
            
            # è¡°å‡ç‡è¶Šè´Ÿï¼Œè¯´æ˜è¡°å‡è¶Šå¿«
            normalized_decay = 1.0 / (1.0 + np.exp(slope))
            
            # ç¬¬ä¸€ä¸ªç‰¹å¾å€¼çš„é›†ä¸­åº¦
            concentration = eigenvalues[0] / np.sum(eigenvalues)
            
            # ç»¼åˆåˆ†æ•°
            spectral_score = 0.6 * normalized_decay + 0.4 * concentration
            
            return np.clip(spectral_score, 0.0, 1.0)
            
        except Exception as e:
            warnings.warn(f"è°±è¡°å‡ç‡è®¡ç®—å‡ºé”™: {e}")
            return 0.5
    
    def participation_ratio_score(self, latent_space):
        """
        å‚ä¸æ¯”åˆ†æ•°
        å¯¹äºè½¨è¿¹æ•°æ®ï¼šè¶Šä½è¶Šå¥½ (ä¿¡æ¯é›†ä¸­)
        å¯¹äºç¨³æ€æ•°æ®ï¼šè¶Šé«˜è¶Šå¥½ (å‡åŒ€åˆ†å¸ƒ)
        """
        try:
            centered_data = latent_space - np.mean(latent_space, axis=0)
            cov_matrix = np.cov(centered_data.T)
            eigenvalues = np.linalg.eigvals(cov_matrix)
            eigenvalues = np.real(eigenvalues)
            eigenvalues = eigenvalues[eigenvalues > 1e-10]
            
            if len(eigenvalues) == 0:
                return 0.0
            
            # å‚ä¸æ¯”å…¬å¼
            sum_eigenvals = np.sum(eigenvalues)
            sum_eigenvals_squared = np.sum(eigenvalues**2)
            
            if sum_eigenvals_squared > 0:
                participation_ratio = sum_eigenvals**2 / sum_eigenvals_squared
                max_participation = len(eigenvalues)
                normalized_pr = participation_ratio / max_participation
            else:
                normalized_pr = 0.0
            
            # æ ¹æ®æ•°æ®ç±»å‹è°ƒæ•´åˆ†æ•°
            if self.participation_preference == "low":
                # è½¨è¿¹æ•°æ®ï¼šä½å‚ä¸æ¯”æ›´å¥½
                score = 1.0 - normalized_pr
            else:
                # ç¨³æ€æ•°æ®ï¼šé«˜å‚ä¸æ¯”æ›´å¥½
                score = normalized_pr
            
            return np.clip(score, 0.0, 1.0)
            
        except Exception as e:
            warnings.warn(f"å‚ä¸æ¯”è®¡ç®—å‡ºé”™: {e}")
            return 0.5
    

    def isotropy_anisotropy_score(self, latent_space):
        """
        å„å‘åŒæ€§/å¼‚æ€§åˆ†æ•° - å¢å¼ºç‰ˆ
        
        å¯¹äºè½¨è¿¹æ•°æ®ï¼šä½å„å‘åŒæ€§æ›´å¥½ (é«˜æ–¹å‘æ€§)
        å¯¹äºç¨³æ€æ•°æ®ï¼šé«˜å„å‘åŒæ€§æ›´å¥½ (å‡åŒ€åˆ†å¸ƒ)
        
        å¢å¼ºç‰¹æ€§ï¼š
        - ä½¿ç”¨å¯¹æ•°å˜æ¢å¢åŠ æ•æ„Ÿåº¦ï¼Œé¿å…é¥±å’Œé—®é¢˜
        - é›†æˆå¤šç§æµ‹é‡æ–¹æ³•æé«˜åŒºåˆ†åº¦
        - åŠ¨æ€è°ƒæ•´æ•æ„Ÿåº¦é˜ˆå€¼
        """
        try:
            centered_data = latent_space - np.mean(latent_space, axis=0)
            cov_matrix = np.cov(centered_data.T)
            eigenvalues = np.linalg.eigvals(cov_matrix)
            eigenvalues = np.real(eigenvalues)
            eigenvalues = eigenvalues[eigenvalues > 1e-12]
            
            if len(eigenvalues) < 2:
                return 1.0
            
            eigenvalues = np.sort(eigenvalues)[::-1]
            
            # æ–¹æ³•1ï¼šå¯¹æ•°æ¤­åœ†åº¦ (è§£å†³é¥±å’Œé—®é¢˜)
            log_ellipticity = np.log(eigenvalues[0]) - np.log(eigenvalues[-1] + 1e-12)
            enhanced_ellipticity = np.tanh(log_ellipticity / 4.0)
            
            # æ–¹æ³•2ï¼šå¤šçº§æ¡ä»¶æ•° (è€ƒè™‘æ‰€æœ‰ç›¸é‚»æ¯”ç‡)
            condition_ratios = []
            for i in range(len(eigenvalues)-1):
                ratio = eigenvalues[i] / (eigenvalues[i+1] + 1e-12)
                condition_ratios.append(np.log(ratio))
            
            mean_log_condition = np.mean(condition_ratios)
            enhanced_condition = np.tanh(mean_log_condition / 2.0)
            
            # æ–¹æ³•3ï¼šæ¯”ç‡æ–¹å·®å„å‘å¼‚æ€§ (é«˜æ•æ„Ÿåº¦)
            ratios = eigenvalues[:-1] / (eigenvalues[1:] + 1e-12)
            ratio_variance = np.var(np.log(ratios))
            ratio_anisotropy = np.tanh(ratio_variance)
            
            # æ–¹æ³•4ï¼šç†µå„å‘å¼‚æ€§
            eigenval_probs = eigenvalues / np.sum(eigenvalues)
            eigenval_entropy = -np.sum(eigenval_probs * np.log(eigenval_probs + 1e-12))
            max_entropy = np.log(len(eigenvalues))
            entropy_isotropy = eigenval_entropy / max_entropy if max_entropy > 0 else 0
            entropy_anisotropy = 1.0 - entropy_isotropy
            
            # æ–¹æ³•5ï¼šä¸»æˆåˆ†æ”¯é…åº¦
            primary_dominance = eigenvalues[0] / np.sum(eigenvalues[1:]) if len(eigenvalues) > 1 else 1
            dominance_anisotropy = np.tanh(np.log(primary_dominance + 1) / 2.0)
            
            # æ–¹æ³•6ï¼šæœ‰æ•ˆç»´åº¦åæ¯”
            participation_ratio = (np.sum(eigenvalues)**2) / np.sum(eigenvalues**2)
            effective_dim_anisotropy = 1.0 - (participation_ratio / len(eigenvalues))
            
            # åŠ æƒç»¼åˆåˆ†æ•°
            anisotropy_components = [
                enhanced_ellipticity * 0.25,      # å¢å¼ºæ¤­åœ†åº¦
                enhanced_condition * 0.25,        # æ”¹è¿›æ¡ä»¶æ•°
                ratio_anisotropy * 0.20,          # æ¯”ç‡æ–¹å·®
                entropy_anisotropy * 0.15,        # ç†µæ–¹æ³•
                dominance_anisotropy * 0.10,      # ä¸»å¯¼æ€§
                effective_dim_anisotropy * 0.05   # æœ‰æ•ˆç»´åº¦
            ]
            
            weighted_anisotropy = np.sum(anisotropy_components)
            
            # æ ¹æ®æ•°æ®ç±»å‹è°ƒæ•´è¾“å‡º
            if self.isotropy_preference == "low":
                # è½¨è¿¹æ•°æ®ï¼šé«˜å„å‘å¼‚æ€§æ›´å¥½
                score = weighted_anisotropy
            else:
                # ç¨³æ€æ•°æ®ï¼šä½å„å‘å¼‚æ€§æ›´å¥½ï¼ˆé«˜å„å‘åŒæ€§ï¼‰
                score = 1.0 - weighted_anisotropy
            
            return np.clip(score, 0.0, 1.0)
            
        except Exception as e:
            warnings.warn(f"å„å‘åŒæ€§åˆ†æå‡ºé”™: {e}")
            return 0.5


    
    # ==================== 3. å•ç»†èƒç‰¹å¼‚æ€§æŒ‡æ ‡ ====================
    
    def trajectory_directionality_score(self, latent_space):
        """
        è½¨è¿¹æ–¹å‘æ€§è¯„ä¼°
        è¯„ä¼°ä¸»å‘è‚²è½´çš„æ”¯é…ç¨‹åº¦
        """
        try:
            pca = PCA()
            pca.fit(latent_space)
            explained_var = pca.explained_variance_ratio_
            
            if len(explained_var) >= 2:
                # ä¸»æ–¹å‘æ”¯é…åº¦
                main_dominance = explained_var[0]
                
                # ç›¸å¯¹äºå…¶ä»–æ–¹å‘çš„æ¯”ç‡
                other_variance = np.sum(explained_var[1:])
                if other_variance > 1e-10:
                    dominance_ratio = explained_var[0] / other_variance
                    # sigmoid å½’ä¸€åŒ–
                    directionality = dominance_ratio / (1.0 + dominance_ratio)
                else:
                    directionality = 1.0
            else:
                directionality = 1.0
                
            return np.clip(directionality, 0.0, 1.0)
            
        except Exception as e:
            warnings.warn(f"è½¨è¿¹æ–¹å‘æ€§è®¡ç®—å‡ºé”™: {e}")
            return 0.5
    
    def noise_resilience_score(self, latent_space):
        """
        å™ªå£°æŠµæŠ—æ€§è¯„ä¼°
        è¯„ä¼°é™ç»´ç»“æœå¯¹æŠ€æœ¯å™ªå£°çš„è¿‡æ»¤èƒ½åŠ›
        """
        try:
            # åŸºäºç‰¹å¾å€¼çš„å™ªå£°è¯„ä¼°
            pca = PCA()
            pca.fit(latent_space)
            explained_variance = pca.explained_variance_
            
            if len(explained_variance) > 1:
                # è®¡ç®—ä¿¡å™ªæ¯”
                signal_variance = np.sum(explained_variance[:2])  # å‰ä¸¤ä¸ªä¸»æˆåˆ†
                noise_variance = np.sum(explained_variance[2:]) if len(explained_variance) > 2 else 0
                
                if noise_variance > 1e-10:
                    snr = signal_variance / noise_variance
                    noise_resilience = min(snr / 10.0, 1.0)  # å½’ä¸€åŒ–
                else:
                    noise_resilience = 1.0  # å®Œç¾å»å™ª
            else:
                noise_resilience = 1.0
                
            return np.clip(noise_resilience, 0.0, 1.0)
            
        except Exception as e:
            warnings.warn(f"å™ªå£°æŠµæŠ—æ€§è®¡ç®—å‡ºé”™: {e}")
            return 0.5
    
    # ==================== 4. ç»¼åˆè¯„ä¼°æ¡†æ¶ ====================
    
    def comprehensive_evaluation(self, latent_space):
        """
        å•ç»†èƒæ•°æ®çš„ç»¼åˆæ½œåœ¨ç©ºé—´è¯„ä¼°
        
        å‚æ•°:
            latent_space: æ½œåœ¨ç©ºé—´åæ ‡
            
        è¿”å›:
            dict: å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        
        self._log(f"å¼€å§‹å•ç»†èƒæ•°æ® ({self.data_type}) ç»¼åˆè¯„ä¼°...")
        
        results = {}
        
        # 1. æ ¸å¿ƒæµå½¢æŒ‡æ ‡
        self._log("è®¡ç®—æµå½¢ç»´åº¦æŒ‡æ ‡...")
        results['manifold_dimensionality'] = self.manifold_dimensionality_score_v2(latent_space)
        
        # 2. è°±åˆ†ææŒ‡æ ‡
        self._log("è®¡ç®—è°±åˆ†ææŒ‡æ ‡...")
        results['spectral_decay_rate'] = self.spectral_decay_rate(latent_space)
        results['participation_ratio'] = self.participation_ratio_score(latent_space)
        results['anisotropy_score'] = self.isotropy_anisotropy_score(latent_space)
        
        # 3. å•ç»†èƒç‰¹å¼‚æ€§æŒ‡æ ‡
        self._log("è®¡ç®—å•ç»†èƒç‰¹å¼‚æ€§æŒ‡æ ‡...")
        results['trajectory_directionality'] = self.trajectory_directionality_score(latent_space)
        
        # 4. æŠ€æœ¯è´¨é‡æŒ‡æ ‡
        self._log("è®¡ç®—æŠ€æœ¯è´¨é‡æŒ‡æ ‡...")
        results['noise_resilience'] = self.noise_resilience_score(latent_space)
        
        # 5. è®¡ç®—ç»¼åˆåˆ†æ•°
        self._log("è®¡ç®—ç»¼åˆåˆ†æ•°...")
        
        # æ ¸å¿ƒè´¨é‡åˆ†æ•° (åŸºç¡€æµå½¢ç‰¹æ€§)
        core_metrics = [
            results['manifold_dimensionality'],
            results['spectral_decay_rate'],
            results['participation_ratio'],
            results['anisotropy_score']
        ]
        results['core_quality'] = np.mean(core_metrics)
        
        # æœ€ç»ˆç»¼åˆåˆ†æ•°
        if self.data_type == "trajectory":
            # è½¨è¿¹æ•°æ®ï¼šæ›´é‡è§†æ–¹å‘æ€§
            final_components = [
                results['core_quality'] * 0.5,          # æ ¸å¿ƒè´¨é‡ 50%
                results['trajectory_directionality'] * 0.3,  # è½¨è¿¹æ–¹å‘æ€§ 30%
                results['noise_resilience'] * 0.2       # å™ªå£°æŠµæŠ— 20%
            ]
        else:
            # ç¨³æ€æ•°æ®ï¼šæ›´é‡è§†æ ¸å¿ƒè´¨é‡
            final_components = [
                results['core_quality'] * 0.7,          # æ ¸å¿ƒè´¨é‡ 70%
                results['noise_resilience'] * 0.3       # å™ªå£°æŠµæŠ— 30%
            ]
        
        results['overall_quality'] = np.sum(final_components)
        
        # æ·»åŠ è§£é‡Šæ€§ä¿¡æ¯
        results['data_type'] = self.data_type
        results['interpretation'] = self._generate_interpretation(results)
        
        if self.verbose:
            self._print_comprehensive_results(results)
        
        return results
    
    def _generate_interpretation(self, results):
        """ç”Ÿæˆç»“æœè§£é‡Š"""
        
        interpretation = {
            'quality_level': '',
            'strengths': [],
            'weaknesses': [],
            'recommendations': []
        }
        
        overall = results['overall_quality']
        
        # è´¨é‡ç­‰çº§
        if overall >= 0.8:
            interpretation['quality_level'] = "ä¼˜ç§€"
        elif overall >= 0.6:
            interpretation['quality_level'] = "è‰¯å¥½"
        elif overall >= 0.4:
            interpretation['quality_level'] = "ä¸­ç­‰"
        else:
            interpretation['quality_level'] = "éœ€è¦æ”¹è¿›"
        
        # åˆ†æå„é¡¹æŒ‡æ ‡
        thresholds = {'high': 0.7, 'medium': 0.5, 'low': 0.3}
        
        # ä¼˜åŠ¿åˆ†æ
        if results['manifold_dimensionality'] > thresholds['high']:
            interpretation['strengths'].append("ç»´åº¦å‹ç¼©æ•ˆç‡é«˜")
        
        if results['spectral_decay_rate'] > thresholds['high']:
            interpretation['strengths'].append("ç‰¹å¾å€¼è¡°å‡è‰¯å¥½")
            
        if results['anisotropy_score'] > thresholds['high']:
            if self.data_type == "trajectory":
                interpretation['strengths'].append("è½¨è¿¹æ–¹å‘æ€§å¼º")
            else:
                interpretation['strengths'].append("ç©ºé—´åˆ†å¸ƒå‡åŒ€")
                
        if results['participation_ratio'] > thresholds['high']:
            if self.data_type == "trajectory":
                interpretation['strengths'].append("ä¿¡æ¯é›†ä¸­åº¦é«˜")
            else:
                interpretation['strengths'].append("ç»´åº¦åˆ©ç”¨å‡è¡¡")
        
        if results['trajectory_directionality'] > thresholds['high']:
            interpretation['strengths'].append("ä¸»å‘è‚²è½´æ˜æ˜¾")
        
        # åŠ£åŠ¿åˆ†æ
        if results['noise_resilience'] < thresholds['medium']:
            interpretation['weaknesses'].append("å™ªå£°è¿‡æ»¤èƒ½åŠ›ä¸è¶³")
            
        if results['trajectory_directionality'] < thresholds['medium']:
            interpretation['weaknesses'].append("ä¸»å‘è‚²è½´ä¸å¤Ÿæ˜æ˜¾")
            
        if results['core_quality'] < thresholds['medium']:
            interpretation['weaknesses'].append("åŸºç¡€æµå½¢è´¨é‡è¾ƒä½")
        
        # å»ºè®®
        if overall < 0.6:
            interpretation['recommendations'].append("è€ƒè™‘è°ƒæ•´é™ç»´å‚æ•°")
            interpretation['recommendations'].append("å¢åŠ æ•°æ®é¢„å¤„ç†æ­¥éª¤")
            
        if results['noise_resilience'] < 0.4:
            interpretation['recommendations'].append("å¢å¼ºå™ªå£°è¿‡æ»¤")
            
        if self.data_type == "trajectory" and results['trajectory_directionality'] < 0.5:
            interpretation['recommendations'].append("ä¼˜åŒ–è½¨è¿¹æ–¹å‘æ€§ä¿æŒ")
        
        return interpretation
    
    def _print_comprehensive_results(self, results):
        """æ‰“å°ç»¼åˆè¯„ä¼°ç»“æœ"""
        
        print("\n" + "="*80)
        print(f"           å•ç»†èƒæ•°æ® ({self.data_type.upper()}) æ½œåœ¨ç©ºé—´è´¨é‡è¯„ä¼°")
        print("="*80)
        
        # æ ¸å¿ƒæŒ‡æ ‡
        print(f"\nã€æ ¸å¿ƒæµå½¢æŒ‡æ ‡ã€‘")
        print(f"  æµå½¢ç»´åº¦ä¸€è‡´æ€§: {results['manifold_dimensionality']:.4f} â˜…")
        print(f"  è°±è¡°å‡ç‡: {results['spectral_decay_rate']:.4f} (è¶Šé«˜è¶Šå¥½)")
        print(f"  å‚ä¸æ¯”åˆ†æ•°: {results['participation_ratio']:.4f} ({'ä½å‚ä¸æ¯”å¥½' if self.participation_preference == 'low' else 'é«˜å‚ä¸æ¯”å¥½'})")
        print(f"  å„å‘å¼‚æ€§åˆ†æ•°: {results['anisotropy_score']:.4f} ({'é«˜å¼‚æ€§å¥½' if self.isotropy_preference == 'low' else 'ä½å¼‚æ€§å¥½'})")
        
        # å•ç»†èƒç‰¹å¼‚æ€§æŒ‡æ ‡
        print(f"\nã€å•ç»†èƒç‰¹å¼‚æ€§æŒ‡æ ‡ã€‘")
        print(f"  è½¨è¿¹æ–¹å‘æ€§: {results['trajectory_directionality']:.4f} (è¶Šé«˜è¶Šå¥½)")
        
        # æŠ€æœ¯è´¨é‡
        print(f"\nã€æŠ€æœ¯è´¨é‡æŒ‡æ ‡ã€‘")
        print(f"  å™ªå£°æŠµæŠ—æ€§: {results['noise_resilience']:.4f} (è¶Šé«˜è¶Šå¥½)")
        
        # ç»¼åˆè¯„ä¼°
        print(f"\nã€ç»¼åˆè¯„ä¼°ã€‘")
        print(f"  æ ¸å¿ƒè´¨é‡åˆ†æ•°: {results['core_quality']:.4f}")
        print(f"  æ€»ä½“è´¨é‡åˆ†æ•°: {results['overall_quality']:.4f} â˜…â˜…â˜…")
        
        # è§£é‡Š
        interp = results['interpretation']
        print(f"\nã€è¯„ä¼°ç»“æœã€‘")
        print(f"  è´¨é‡ç­‰çº§: {interp['quality_level']}")
        
        if interp['strengths']:
            print(f"  ä¼˜åŠ¿: {', '.join(interp['strengths'])}")
        
        if interp['weaknesses']:
            print(f"  åŠ£åŠ¿: {', '.join(interp['weaknesses'])}")
            
        if interp['recommendations']:
            print(f"  å»ºè®®: {', '.join(interp['recommendations'])}")
        
        print("="*80)
    
    def compare_methods(self, method_results_dict):
        """
        æ¯”è¾ƒä¸åŒé™ç»´æ–¹æ³•çš„æ•ˆæœ
        
        å‚æ•°:
            method_results_dict: {method_name: latent_space} å­—å…¸
            
        è¿”å›:
            DataFrame: æ¯”è¾ƒç»“æœè¡¨æ ¼
        """
        
        comparison_results = []
        
        for method_name, latent_space in method_results_dict.items():
            self._log(f"\nè¯„ä¼°æ–¹æ³•: {method_name}")
            
            # æš‚æ—¶å…³é—­è¯¦ç»†è¾“å‡º
            original_verbose = self.verbose
            self.verbose = False
            
            results = self.comprehensive_evaluation(latent_space)
            
            # æ¢å¤è¾“å‡ºè®¾ç½®
            self.verbose = original_verbose
            
            # æå–å…³é”®æŒ‡æ ‡
            comparison_results.append({
                'Method': method_name,
                'Overall_Quality': results['overall_quality'],
                'Manifold_Dimensionality': results['manifold_dimensionality'],
                'Spectral_Decay': results['spectral_decay_rate'],
                'Participation_Ratio': results['participation_ratio'],
                'Anisotropy_Score': results['anisotropy_score'],
                'Trajectory_Directionality': results['trajectory_directionality'],
                'Noise_Resilience': results['noise_resilience'],
                'Quality_Level': results['interpretation']['quality_level']
            })
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(comparison_results)
        
        # æŒ‰æ€»ä½“è´¨é‡æ’åº
        df = df.sort_values('Overall_Quality', ascending=False)
        
        if self.verbose:
            self._print_comparison_table(df)
        
        return df
    
    def _print_comparison_table(self, df):
        """æ‰“å°æ¯”è¾ƒç»“æœè¡¨æ ¼"""
        
        print(f"\n{'='*100}")
        print(f"                    é™ç»´æ–¹æ³•æ•ˆæœæ¯”è¾ƒ ({self.data_type.upper()} æ•°æ®)")
        print('='*100)
        
        # è®¾ç½®æ˜¾ç¤ºæ ¼å¼
        pd.set_option('display.float_format', '{:.4f}'.format)
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        
        print(df.to_string(index=False))
        
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {df.iloc[0]['Method']} (æ€»åˆ†: {df.iloc[0]['Overall_Quality']:.4f})")
        
        print('='*100)

# ==================== ä¾¿æ·å‡½æ•° ====================

def evaluate_single_cell_latent_space(latent_space, data_type="trajectory", verbose=True):
    """
    ä¾¿æ·å‡½æ•°ï¼šè¯„ä¼°å•ç»†èƒæ½œåœ¨ç©ºé—´è´¨é‡
    
    å‚æ•°:
        latent_space: æ½œåœ¨ç©ºé—´åæ ‡
        data_type: "trajectory" æˆ– "steady_state"  
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        
    è¿”å›:
        dict: è¯„ä¼°ç»“æœ
    """
    
    evaluator = SingleCellLatentSpaceEvaluator(data_type=data_type, verbose=verbose)
    return evaluator.comprehensive_evaluation(latent_space)

def compare_single_cell_methods(method_results_dict, data_type="trajectory", verbose=True):
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¯”è¾ƒä¸åŒå•ç»†èƒé™ç»´æ–¹æ³•
    
    å‚æ•°:
        method_results_dict: {method_name: latent_space} å­—å…¸
        data_type: "trajectory" æˆ– "steady_state"
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        
    è¿”å›:
        DataFrame: æ¯”è¾ƒç»“æœ
    """
    
    evaluator = SingleCellLatentSpaceEvaluator(data_type=data_type, verbose=verbose)
    return evaluator.compare_methods(method_results_dict)
